// Copyright 2023 ONDEWO GmbH
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.https://ondewo.slack.com/archives/CAWPP61NY

syntax = "proto3";

package ondewo.s2t;
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";

// <p>Speech-to-text service</p>
service Speech2Text {

    // <p>Transcribes an audio file</p>
    rpc TranscribeFile (TranscribeFileRequest) returns (TranscribeFileResponse) {};

    // <p>Transcribes an audio stream.</p>
    rpc TranscribeStream (stream TranscribeStreamRequest) returns (stream TranscribeStreamResponse) {};

    // <p>Gets a speech to text pipeline corresponding to the id specified in <code>S2tPipelineId</code>. If no corresponding id is
    // found, raises <code>ModuleNotFoundError</code> in server.</p>
    rpc GetS2tPipeline (S2tPipelineId) returns (Speech2TextConfig) {};

    // <p>Creates a new speech to text pipeline from a <code>Speech2TextConfig</code> and registers the new pipeline in the server.</p>
    rpc CreateS2tPipeline (Speech2TextConfig) returns (S2tPipelineId) {};

    // <p>Deletes a pipeline corresponding to the id parsed in <code>S2tPipelineId</code>. If no corresponding id is
    // found, raises <code>ModuleNotFoundError</code> in server.</p>
    rpc DeleteS2tPipeline (S2tPipelineId) returns (google.protobuf.Empty) {};

    // <p>Updates a pipeline with the id specified in <code>Speech2TextConfig</code> with the new config. If no corresponding id is
    // found, raises <code>ModuleNotFoundError</code> in server.</p>
    rpc UpdateS2tPipeline (Speech2TextConfig) returns (google.protobuf.Empty) {};

    // <p>Lists all speech to text pipelines.</p>
    rpc ListS2tPipelines (ListS2tPipelinesRequest) returns (ListS2tPipelinesResponse) {};

    // <p>Returns a message containing a list of all languages for which there exist pipelines.</p>
    rpc ListS2tLanguages (ListS2tLanguagesRequest) returns (ListS2tLanguagesResponse) {};

    // <p>Returns a message containing a list of all domains for which there exist pipelines.</p>
    rpc ListS2tDomains (ListS2tDomainsRequest) returns (ListS2tDomainsResponse) {};

    // <p>Returns a message containing the version of the running speech to text server.</p>
    rpc GetServiceInfo (google.protobuf.Empty) returns (S2tGetServiceInfoResponse) {};

    // <p>Given a list of pipeline ids, returns a list of <code>LanguageModelPipelineId</code> messages containing the pipeline
    // id and a list of the language models loaded in the pipeline.</p>
    rpc ListS2tLanguageModels (ListS2tLanguageModelsRequest) returns (ListS2tLanguageModelsResponse) {};

    // <p>Create a user language model.</p>
    rpc CreateUserLanguageModel (CreateUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // <p>Delete a user language model.</p>
    rpc DeleteUserLanguageModel (DeleteUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // <p>Add data to a user language model.</p>
    rpc AddDataToUserLanguageModel (AddDataToUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // <p>Train a user language model.</p>
    rpc TrainUserLanguageModel (TrainUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // <p>Retrieves a list of normalization pipelines based on specific requirements.</p>
    rpc ListS2tNormalizationPipelines(ListS2tNormalizationPipelinesRequest) returns (ListS2tNormalizationPipelinesResponse);

}

///////////////////////////
//         Enums         //
///////////////////////////

// The decoding configuration
enum Decoding {

    // decoding will be defined by the pipeline config
    DEFAULT = 0;

    // greedy decoding will be used independently on pipeline config
    GREEDY = 1;

    // beam search will be used independently on pipeline config
    BEAM_SEARCH_WITH_LM = 2;

    // beam search without LM head, to configure decoding mode for seq2seq models.
    BEAM_SEARCH = 3;

}

// The inference backend configuration
enum InferenceBackend {

    // Not set
    INFERENCE_BACKEND_UNKNOWN = 0;

    // Run pytorch model
    INFERENCE_BACKEND_PYTORCH = 1;

    // Run flax model
    INFERENCE_BACKEND_FLAX = 2;

    // Run Amazon S2T cloud service
    INFERENCE_BACKEND_CLOUD_SERVICE_AMAZON = 3;

    // Run Deepgram S2T cloud service
    INFERENCE_BACKEND_CLOUD_SERVICE_DEEPGRAM = 4;

    // Run Google S2T cloud service
    INFERENCE_BACKEND_CLOUD_SERVICE_GOOGLE = 5;

    // Run Microsoft Azure S2T cloud service
    INFERENCE_BACKEND_CLOUD_SERVICE_MICROSOFT = 6;

}

///////////////////////////
// Configuration Message //
///////////////////////////

// <p>Configuration for a request to transcribe audio</p>
message TranscribeRequestConfig {

    // Required. id of the pipeline (model setup) that will generate audio
    string s2t_pipeline_id = 1;

    // Optional. decoding type
    Decoding decoding = 2;

    // Optional. Language model name for decoding (in none, default will be taken from config file)
    oneof oneof_language_model_name {

        // Name of the language model
        string language_model_name = 3;
    }

    // Optional. Configuration of spelling correction and normalization
    oneof oneof_post_processing {

        // The postprocessing options
        PostProcessingOptions post_processing = 4;
    }

    // Optional. Configuration for the detection of utterances
    oneof oneof_utterance_detection {

        // The utterance detection options
        UtteranceDetectionOptions utterance_detection = 5;
    }

    // Optional. Which method to be used for voice activity detection
    oneof voice_activity_detection {

        // Voice activity detection with pyannote
        Pyannote pyannote = 6;
    }

    // Optional. Specify which data to return
    oneof oneof_return_options {

        // The transcribe return options
        TranscriptionReturnOptions return_options = 8;
    }

    // Optional. Specify language of transcription to return
    optional string language = 9;

    // Optional. Specify task of s2t model, e.g. &apos;transcribe&apos; and &apos;translate&apos;
    optional string task = 10;

    // Optional. <code>s2t_service_config</code> provides the configuration of the service such as API key, bearer tokens, JWT,
    // and other header information as key value pairs, e.g., <pre><code>MY_API_KEY=&apos;LKJDIFe244LKJOI&apos;</code></pre>
    // For Amazon S2T service, the following arguments should be passed in form of a dict:
    // <ul>
    //   <li><code>aws_access_key_id</code> (required) Access key id to access Amazon Web Service.</li>
    //   <li><code>aws_secret_access_key</code> (required) Secret access key to access Amazon Web Service.</li>
    //   <li><code>region</code> (required) Region name of Amazon Server.</li>
    // </ul>
    // Example: <code>s2t_config_service={&apos;aws_access_key_id&apos;: &apos;YOUR_AWS_ACCESS_KEY_ID&apos;, &apos;aws_secret_access_key&apos;: 
    // &apos;YOUR_AWS_SECRET_ACCESS_KEY&apos;, &apos;region&apos;: &apos;YOUR_AMAZON_SERVER_REGION_NAME&apos;}</code>
    // For Deepgram S2T service, the following argument should be passed in form of a dict:
    // <ul>
    //   <li><code>api_key</code> (required) API key of Deepgram account to access Deepgram S2T service.</li>
    // </ul>
    // Example: <code>s2t_config_service={&apos;api_key&apos;: &apos;YOUR_DEEPGRAM_API_KEY&apos;}</code>
    // <br>
    // For Google cloud S2T service, the following arguments should be passed in form of a dict:
    // <ul>
    //   <li><code>api_key</code> (required) API key of Google cloud to access its S2T service.</li>
    //   <li><code>api_endpoint</code> (optional) Regional API endpoint of Google cloud S2T service. (Defaults to 
    // &apos;eu-speech.googleapis.com&apos;)</li>
    // </ul>
    // Example: <code>s2t_config_service={&apos;api_key&apos;: &apos;YOUR_GOOGLE_CLOUD_API_KEY&apos;, &apos;api_endpoint&apos;: &apos;YOUR_GOOGLE_CLOUD_API_ENDPOINT&apos;}</code>
    // For Microsoft Azure S2T service, the following arguments should be passed in form of a dict:
    // <ul>
    //   <li><code>subscription_key</code> (required) Subscription key to access Microsoft Azure Service.</li>
    //   <li><code>region</code> (required) Region name of Microsoft Azure Server.</li>
    // </ul>
    // Example: <code>s2t_config_service={&apos;subscription_key&apos;: &apos;YOUR_MICROSOFT_AZURE_SUBSCRIPTION_KEY&apos;, &apos;region&apos;: 
    // &apos;YOUR_MICROSOFT_AZURE_SERVER_REGION_NAME&apos;}</code>
    // Note: ondewo-s2t will raise an error if you don&apos;t pass any of the required arguments above.
    optional google.protobuf.Struct s2t_service_config = 11;

    // Optional. Defines the cloud provider&apos;s specific configuration for using speech to text cloud services.
    // The default value is None.
    optional S2tCloudProviderConfig s2t_cloud_provider_config = 12;

}

// <p>Configuration for cloud provider settings for Speech-to-Text (S2T).</p>
message S2tCloudProviderConfig {

    // Optional. Configuration for Amazon web service speech-to-text provider.
    optional S2tCloudProviderConfigAmazon s2t_cloud_provider_config_amazon = 1;

    // Optional. Configuration for DeepGram speech-to-text provider.
    optional S2tCloudProviderConfigDeepgram s2t_cloud_provider_config_deepgram = 2;

    // Optional. Configuration for Google speech-to-text provider.
    optional S2tCloudProviderConfigGoogle s2t_cloud_provider_config_google = 3;

    // Optional. Configuration for Microsoft Azure speech-to-text provider.
    optional S2tCloudProviderConfigMicrosoft s2t_cloud_provider_config_microsoft = 4;

}

// <p>Configuration details specific to the Amazon web service speech-to-text provider.</p>
message S2tCloudProviderConfigAmazon {

    // Optional. Enables or disables <code>partial_results_stabilization</code> feature. More details at:
    // <a href="https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization">https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization</a>
    optional bool enable_partial_results_stabilization = 1;

    // Optional. You can use this field to set the stability level of the transcription results.
    // A higher stability level means that the transcription results are less likely to change.
    // Higher stability levels can come with lower overall transcription accuracy.
    // Acceptable values: [&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;]. Defaults to &quot;high&quot; if not set explicitly. More details at:
    // <a href="https://aws.amazon.com/blogs/machine-learning/amazon-transcribe-now-supports-partial-results-stabilization-for-streaming-audio/">https://aws.amazon.com/blogs/machine-learning/amazon-transcribe-now-supports-partial-results-stabilization-for-streaming-audio/</a>
    optional string partial_results_stability = 2;

    // Optional. The name of your customize language model you want to use.
    // More details at: <a href="https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html">https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html</a>
    optional string language_model_name = 3;

    // Optional. The name of your customize vocabulary you want to use.
    // More details at: <a href="https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html">https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html</a>
    optional string vocabulary_name = 4;

}

// <p>Configuration details specific to the Deepgram speech-to-text provider.</p>
message S2tCloudProviderConfigDeepgram {

    // Optional. Enables or disables <code>punctuate</code> feature of Deepgram to add punctuations to the resulted transcript.
    // More details at: <a href="https://developers.deepgram.com/docs/punctuation">https://developers.deepgram.com/docs/punctuation</a>
    optional bool punctuate = 1;

    // Optional. Enables or disables <code>smart_format</code> feature of Deepgram transcription result to improve readability.
    // More details at: <a href="https://developers.deepgram.com/docs/smart-format">https://developers.deepgram.com/docs/smart-format</a>
    optional bool smart_format = 2;

    // Optional. Enables or disables <code>numerals</code> feature of Deepgram to convert numbers to numeric form in the resulted 
    // transcript. More details at: <a href="https://developers.deepgram.com/docs/numerals">https://developers.deepgram.com/docs/numerals</a>
    optional bool numerals = 3;

    // Optional. Enables or disables <code>measurements</code> feature of Deepgram to convert measurement units (i.e. Kilogram)
    // to abbreviated form (i.e. Kg) in the resulted transcript.
    // More details at: <a href="https://developers.deepgram.com/docs/measurements">https://developers.deepgram.com/docs/measurements</a>
    optional bool measurements = 4;

    // Optional. Enables or disables <code>dictation</code> feature of Deepgram to convert spoken dictation commands into their 
    // corresponding punctuation marks. More details at: <a href="https://developers.deepgram.com/docs/dictation">https://developers.deepgram.com/docs/dictation</a>
    optional bool dictation = 5;

}

// <p>Configuration details specific to the Google speech-to-text provider.</p>
message S2tCloudProviderConfigGoogle {

    // Optional. Enables or disables <code>automatic_punctuation</code> feature of Google s2t to add punctuations to the resulted 
    // transcript. More details at: <a href="https://cloud.google.com/speech-to-text/docs/automatic-punctuation">https://cloud.google.com/speech-to-text/docs/automatic-punctuation</a>
    optional bool enable_automatic_punctuation = 1;

    // Optional. Enables or disables <code>word_time_offsets</code> feature of Google s2t to add word-level timestamps (time-offset)
    // to the resulted transcript. More details at: <a href="https://cloud.google.com/speech-to-text/docs/async-time-offsets">https://cloud.google.com/speech-to-text/docs/async-time-offsets</a>
    optional bool enable_word_time_offsets = 2;

    // Optional. Enables or disables <code>word_confidence</code> feature of Google s2t to add word-level confidence scores
    // to the resulted transcript. More details at: <a href="https://cloud.google.com/speech-to-text/docs/word-confidence">https://cloud.google.com/speech-to-text/docs/word-confidence</a>
    optional bool enable_word_confidence = 3;

    // Optional. Enables or disables <code>transcript_normalization</code> feature of Google s2t to automatically
    // replace parts of the transcript with phrases of your choosing. More details at:
    // <a href="https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization">https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization</a>
    optional bool transcript_normalization = 4;

    // Optional. Maximum number of recognition hypotheses to be returned, may be returned fewer than <code>max_alternatives</code>.
    // Valid values are 0-30. A value of 0 or 1 will return a maximum of one. If omitted, will return a maximum of one.
    optional int32 max_alternatives = 5;

}

// <p>Configuration details specific to the Microsoft Azure speech-to-text provider.</p>
message S2tCloudProviderConfigMicrosoft {

    // Optional. Enables or disables the Microsoft Azure fast transcription API. It is faster than SDK but is in 
    // preview version.
    // More details at: <a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create">https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create</a>
    optional bool use_fast_transcription_api = 1;

    // Optional. Enables or disables the <code>detailed</code> format for the result of Microsoft Azure s2t service
    // to add timestamps and confidences to the resulted transcript.
    optional bool use_detailed_output_format = 2;

}

// <p>Configuration of the return values of a transcribe request</p>
message TranscriptionReturnOptions {

    // should server make response indicating that the beginning of the speech was detected
    bool return_start_of_speech = 1;

    // should s2t server return audio bytes of transcribed utterance
    bool return_audio = 2;

    // Whether or not to return confidence scores
    bool return_confidence_score = 3;

    // Whether or not to return alternative results from beam-search
    bool return_alternative_transcriptions = 4;

    // Optional. Number of alternative transcriptions results from beam-search or greedy-search
    int32 return_alternative_transcriptions_nr = 5;

    // Whether or not to return alternative results from beam-search
    bool return_alternative_words = 6;

    // Optional. Number of alternative words to results
    int32 return_alternative_words_nr = 7;

    // Optional. Whether or not to return timestamps of start and end of the words. Only used in TranscribeFile.
    bool return_word_timing = 8;

}

// <p>Configuration of the options to detect utterances</p>
message UtteranceDetectionOptions {

    // Whether or not to transcribe unfinished utterances.
    oneof oneof_transcribe_not_final {

        // Return also immediate transcription results
        bool transcribe_not_final = 1;
    }

    // if time between audio chunks exceeds next_chunk_timeout, stream will be stopped
    float next_chunk_timeout = 2;
    
    // Optional. Configuration for the turn-detection in utterances
    oneof oneof_turn_detection {

        // The turn detection options
        TurnDetectionOptions turn_detection = 3;
    }
}

// <p>Configuration of the post-processing options</p>
message PostProcessingOptions {
    
    // Whether to use spelling correction
    bool spelling_correction = 1;
    
    // Whether to disable normalization
    bool normalize = 2;
    
    // Post-processing configuration specifying the active post-processors in the pipeline, as well as their individual
    // configuration. If not set, all values are replaced by the ones in current pipeline.
    PostProcessing config = 3;
    
    // Whether to disable LLM post-processing
    bool llm_post_processing = 4;

}

///////////////////////////
//  TRANSCRIPTION TYPE  //
///////////////////////////

// <p>The transcription message</p>
message Transcription {

    // The transcribed text
    string transcription = 1;

    // The corresponding confidence score. The confidence estimate between 0.0 and 1.0. A higher number
    // indicates an estimated greater likelihood that the recognized words are correct.
    float confidence_score = 2;

    // List of the words of transcription with their confidence scores and probable alternatives
    repeated WordDetail words = 3;

    // List of alternative transcriptions, confidence scores, words timings and alternative words
    repeated TranscriptionAlternative alternatives = 4;

}

// <p>The alternative transcription message</p>
message TranscriptionAlternative{

    // The alternative transcribed text
    string transcript = 1;

    // The corresponding confidence score to the alternative transcript.
    float confidence = 2;

    // A list of word-specific information for each recognized word, including word timings, confidence score of
    // the word and alternative words.
    repeated WordDetail words = 3;

}

// <p>WordDetail provides word-specific information for recognized words.</p>
message WordDetail {

    // The start time of the spoken word relative to the beginning of the audio.
    // The accuracy of the time offset can vary, and this is an experimental feature.
    float start_time = 1;

    // The end time of the spoken word relative to the beginning of the audio.
    // The accuracy of the time offset can vary, and this is an experimental feature.
    float end_time = 2;

    // The recognized word corresponding to this set of information.
    string word = 3;

    // The corresponding confidence score to the word.
    float confidence = 4;

    // List of alternative words and confidence scores of each.
    repeated WordAlternative word_alternatives = 5;

}

// <p>The word alternative message</p>
message WordAlternative{

    // The recognized word corresponding to this set of information.
    string word = 1;

    // The corresponding confidence score to the alternative word.
    float confidence = 2;

}

///////////////////////
// TRANSCRIBE STREAM //
///////////////////////

// <p>Request to transcribe an audio stream</p>
message TranscribeStreamRequest {

    // wav file to transcribe
    bytes audio_chunk = 1;

    // if it's the final chunk of the stream
    bool end_of_stream = 2;

    // The configuration to override the default configuration
    TranscribeRequestConfig config = 3;

    // Whether or not to mute the audio signal. Defaults to false.
    bool mute_audio = 4;

}

// <p>The response message of a stream transcription</p>
message TranscribeStreamResponse {

    // List of transcriptions with confidence level
    repeated Transcription transcriptions = 1;

    // The time the transcription took
    float time = 2;

    // Whether or not this transcription is final (transcribed texts might change if transcription is
    // started before the end of an utterance).
    bool final = 3;

    // is audio bytes of the utterance in response
    bool return_audio = 4;

    // audio bytes of the transcribed utterance
    bytes audio = 5;

    // is it a start of the utterance
    bool utterance_start = 6;

    // id of the transcribed audio file
    string audio_uuid = 7;

    // The configuration to override the default configuration
    oneof oneof_config {

        // The configuration for the transcription
        TranscribeRequestConfig config = 8;
    }

}

/////////////////////
// TRANSCRIBE FILE //
/////////////////////

// <p>A request to transcribe an audio file</p>
message TranscribeFileRequest {

    // wav file to transcribe
    bytes audio_file = 1;

    // The configuration to override the default configuration
    TranscribeRequestConfig config = 2;

}

// <p>The response message for a transcribe file request</p>
message TranscribeFileResponse {

    // List of transcriptions with confidence level
    repeated Transcription transcriptions = 1;

    // The time the transcription took
    float time = 2;

    // id of the transcribed audio file
    string audio_uuid = 3;

}

//////////////////////
// GET S2T PIPELINE //
//////////////////////

// <p>The pipeline id for a specific pipeline configuration</p>
message S2tPipelineId {

    // id of the model that will generate audio
    string id = 1;

}

////////////////////////
// LIST S2T PIPELINES //
////////////////////////

// <p>Request to list all speech-to-text pipelines. Optionally also filter criteria can be set</p>
message ListS2tPipelinesRequest {

    // Filter for languages
    repeated string languages = 1;

    // Filter for pipeline owners
    repeated string pipeline_owners = 2;

    // Filter for domains
    repeated string domains = 3;

    // If true, return only registered pipelines.
    // Default false: return registered and persisted (from config files) configs.
    bool registered_only = 4;

}

// <p>ListS2tPipelinesResponse is used to return a list of all speech-to-text pipelines.</p>
message ListS2tPipelinesResponse {

    // A list of <code>Speech2TextConfig</code> message instances containing the configuration of each pipeline.
    // Example: [{id: &quot;pipeline_1&quot;, description: {language: &quot;en&quot;}, active: true, ...}, {id: &quot;pipeline_2&quot;,
    // description: {language: &quot;fr&quot;}, active: true, ...}]
    repeated Speech2TextConfig pipeline_configs = 1;
}

////////////////////////
// LIST S2T LANGUAGES //
////////////////////////

// <p>ListS2tLanguagesRequest is used to request a list of available languages. Optionally, filters can be set.</p>
message ListS2tLanguagesRequest {

    // Filter for domains.  Example: [&quot;medical&quot;, &quot;finance&quot;]
    repeated string domains = 1;

    // Filter for pipeline owners.
    // Example: [&quot;ondewo&quot;, &quot;partner_company&quot;]
    repeated string pipeline_owners = 2;

}

// <p>Response message to list available languages</p>
message ListS2tLanguagesResponse {

    // available languages
    repeated string languages = 1;

}

//////////////////////
// LIST S2T DOMAINS //
//////////////////////

// <p>Request message to list available domains. Optionally also filters can be set.</p>
message ListS2tDomainsRequest {

    // Filter for languages
    repeated string languages = 1;

    // Filter for pipeline owner
    repeated string pipeline_owners = 2;

}

// <p>Response message to list available domains</p>
message ListS2tDomainsResponse {

    // domains available. Example: [&quot;medical&quot;, &quot;finance&quot;]
    repeated string domains = 1;

}

//////////////////////
// GET SERVICE INFO //
//////////////////////

// <p>S2tGetServiceInfoResponse is used to return version information about the speech-to-text service.</p>
message S2tGetServiceInfoResponse {

    // Version number based on semantic versioning, e.g. &quot;4.2.0&quot;.
    string version = 1;
}

///////////////////////////////////
// SPEECH-2-TEXT PIPELINE CONFIG //
///////////////////////////////////

// <p>Speech2TextConfig is a configuration message for the speech-to-text pipeline</p>
message Speech2TextConfig {

    // Unique identifier for the configuration.
    string id = 1;

    // Description of the speech-to-text system.
    S2tDescription description = 2;

    // Indicates if the configuration is active.
    bool active = 3;

    // Configuration for inference models.
    S2tInference inference = 4;

    // Configuration for the streaming server.
    StreamingServer streaming_server = 5;

    // Configuration for voice activity detection.
    VoiceActivityDetection voice_activity_detection = 6;

    // Configuration for post-processing.
    PostProcessing post_processing = 7;

    // Configuration for logging.
    Logging logging = 8;

}


// <p>S2tDescription contains descriptive information about the speech-to-text pipeline.</p>
message S2tDescription {

    // Language of the speech-to-text system.
    string language = 1;

    // Owner of the pipeline.
    string pipeline_owner = 2;

    // Domain of the speech-to-text system.
    string domain = 3;

    // Comments about the system.
    string comments = 4;

}


// <p>S2tInference contains information about inference models used in the speech-to-text pipeline.</p>
message S2tInference {

    // Configuration for the acoustic models.
    AcousticModels acoustic_models = 1;

    // Configuration for the language models.
    LanguageModels language_models = 2;

    // Configuration for the inference backend.
    InferenceBackend inference_backend = 3;

}

// <p>AcousticModels contains information about different types of acoustic models.</p>
message AcousticModels {

    // Type of the acoustic model.
    string type = 1;

    // Configuration for the Wav2Vec model.
    Wav2Vec wav2vec = 2;

    // Configuration for the Wav2Vec model using Triton.
    Wav2VecTriton wav2vec_triton = 3;

    // Configuration for the Whisper model.
    Whisper whisper = 4;

    // Configuration for the Whisper model using Triton.
    WhisperTriton whisper_triton = 5;

    // Amazon web service cloud service inference settings.
    S2tCloudServiceAmazon s2t_cloud_service_amazon = 6;

    // Deepgram cloud service inference settings.
    S2tCloudServiceDeepgram s2t_cloud_service_deepgram = 7;

    // Google cloud service inference settings.
    S2tCloudServiceGoogle s2t_cloud_service_google = 8;

    // Microsoft Azure cloud service inference settings.
    S2tCloudServiceMicrosoft s2t_cloud_service_microsoft = 9;

}

// <p>S2tCloudServiceAmazon message contains settings for the Amazon web service Cloud service inference.</p>
message S2tCloudServiceAmazon {

    // Language of the audio to transcribe by Amazon web service s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. &apos;en-US&apos; or &apos;de-DE&apos;.
    string language = 1;

    // Specifies if streaming mode of Amazon web service speech to text is available for the selected language, 
    // otherwise batch mode transcription is used. See the list of languages and available transcription modes at:
    // <a href="https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html">https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html</a>
    bool streaming_available = 2;

    // Enables or disables <code>partial_results_stabilization</code> feature. More details at:
    // <a href="https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization">https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization</a>
    bool enable_partial_results_stabilization = 3;

    // You can use this field to set the stability level of the transcription results.
    // A higher stability level means that the transcription results are less likely to change.
    // Higher stability levels can come with lower overall transcription accuracy.
    // Defaults to &quot;high&quot; if not set explicitly.
    string partial_results_stability = 4;

    // The name of your customize language model you want to use.
    // More details at: <a href="https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html">https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html</a>
    string language_model_name = 5;

    // The name of your customize vocabulary you want to use.
    // More details at: <a href="https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html">https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html</a>
    string vocabulary_name = 6;

}

// <p>S2tCloudServiceDeepgram message contains settings for the Deepgram Cloud service inference.</p>
message S2tCloudServiceDeepgram {

    // Model name from one of the speech-to-text models provided by Deepgram for the desired use-case.
    // Provided model names and details at: <a href="https://developers.deepgram.com/docs/model">https://developers.deepgram.com/docs/model</a>
    string model_name = 1;

    // Language of the audio to transcribe by Deepgram s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. &apos;en-US&apos; or &apos;de-DE&apos;.
    string language = 2;

    // Enables or disables <code>punctuate</code> feature of Deepgram to add punctuations to the resulted transcript.
    // More details at: <a href="https://developers.deepgram.com/docs/punctuation">https://developers.deepgram.com/docs/punctuation</a>
    bool punctuate = 3;

    // Enables or disables <code>smart_format</code> feature of Deepgram transcription result to improve readability.
    // More details at: <a href="https://developers.deepgram.com/docs/smart-format">https://developers.deepgram.com/docs/smart-format</a>
    bool smart_format = 4;

    // Enables or disables <code>numerals</code> feature of Deepgram to convert numbers to numeric form in the resulted transcript.
    // More details at: <a href="https://developers.deepgram.com/docs/numerals">https://developers.deepgram.com/docs/numerals</a>
    bool numerals = 5;

    // Enables or disables <code>measurements</code> feature of Deepgram to convert measurement units (i.e. Kilogram)
    // to abbreviated form (i.e. Kg) in the resulted transcript.
    // More details at: <a href="https://developers.deepgram.com/docs/measurements">https://developers.deepgram.com/docs/measurements</a>
    bool measurements = 6;

    // Enables or disables <code>dictation</code> feature of Deepgram to convert spoken dictation commands into their corresponding
    // punctuation marks. More details at: <a href="https://developers.deepgram.com/docs/dictation">https://developers.deepgram.com/docs/dictation</a>
    bool dictation = 7;

}

// <p>S2tCloudServiceGoogle message contains settings for the Google Cloud service inference.</p>
message S2tCloudServiceGoogle {

    // Model name from one of the speech-to-text models provided by Google for the desired use-case.
    // Provided model names and details at: <a href="https://cloud.google.com/speech-to-text/docs/transcription-model">https://cloud.google.com/speech-to-text/docs/transcription-model</a>
    string model_name = 1;

    // Language of the audio to transcribe by Google s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. &apos;en-US&apos; or &apos;de-DE&apos;.
    string language = 2;

    // Enables or disables <code>automatic_punctuation</code> feature of Google s2t to add punctuations to the resulted transcript.
    // More details at: <a href="https://cloud.google.com/speech-to-text/docs/automatic-punctuation">https://cloud.google.com/speech-to-text/docs/automatic-punctuation</a>
    bool enable_automatic_punctuation = 3;

    // Enables or disables <code>word_time_offsets</code> feature of Google s2t to add word-level timestamps (time-offsets)
    // to the resulted transcript. More details at: <a href="https://cloud.google.com/speech-to-text/docs/async-time-offsets">https://cloud.google.com/speech-to-text/docs/async-time-offsets</a>
    bool enable_word_time_offsets = 4;

    // Enables or disables <code>word_confidence</code> feature of Google s2t to add word-level confidence scores
    // to the resulted transcript. More details at: <a href="https://cloud.google.com/speech-to-text/docs/word-confidence">https://cloud.google.com/speech-to-text/docs/word-confidence</a>
    bool enable_word_confidence = 5;

    // Enables or disables <code>transcript_normalization</code> feature of Google s2t to automatically
    // replace parts of the transcript with phrases of your choosing. More details at:
    // <a href="https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization">https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization</a>
    bool transcript_normalization = 6;

    // Maximum number of recognition hypotheses to be returned. The server may return fewer than <code>max_alternatives</code>.
    // Valid values are 0-30. A value of 0 or 1 will return a maximum of one. If omitted, will return a maximum of one.
    int32 max_alternatives = 7;

}

// <p>S2tCloudServiceMicrosoft message contains settings for the Microsoft Azure Cloud service inference.</p>
message S2tCloudServiceMicrosoft {

    // Language of the audio to transcribe by Microsoft Azure s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. &apos;en-US&apos; or &apos;de-DE&apos;.
    string language = 1;

    // Enables or disables the Microsoft Azure fast transcription API. It is faster than SDK but is in preview version.
    // More details at: <a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create">https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create</a>
    bool use_fast_transcription_api = 2;

    // Enables or disables the <code>detailed</code> format for the result of Microsoft Azure s2t service
    // to add timestamps and confidences to the resulted transcript.
    bool use_detailed_output_format = 3;

}

// <p>Whisper contains information about the Whisper model.</p>
message Whisper {

    // Path to the model.
    string model_path = 1;

    // Indicates if GPU is used.
    bool use_gpu = 2;

    // Default language of the model.
    string language = 3;

    // Default task of the model.
    string task = 4;

}

// <p>WhisperTriton contains information about the Whisper model using Triton.</p>
message WhisperTriton {

    // Path to the processor.
    string processor_path = 1;

    // Name of the Triton model.
    string triton_model_name = 2;

    // Version of the Triton model.
    string triton_model_version = 3;

    // Timeout for checking model status.
    int64 check_status_timeout = 4;

    // Default language of the model.
    string language = 5;

    // Default task of the model. E.g., transcribe, translate, etc.
    string task = 6;

    // Host name of triton inference server that serves the WhisperTriton model
    string triton_server_host = 7;

    // Port number of triton inference server that serves the WhisperTriton model
    int64 triton_server_port = 8;

}

// <p>Wav2Vec contains information about the Wav2Vec model.</p>
message Wav2Vec {

    // Path to the model.
    string model_path = 1;

    // Indicates if GPU is used.
    bool use_gpu = 2;

}

// <p>Wav2VecTriton contains information about the Wav2Vec model using Triton.</p>
message Wav2VecTriton {

    // Path to the processor.
    string processor_path = 1;

    // Name of the Triton model.
    string triton_model_name = 2;

    // Version of the Triton model.
    string triton_model_version = 3;

    // Timeout for checking model status.
    int64 check_status_timeout = 4;

    // Host name of triton inference server that serves the Wav2VecTriton model
    string triton_server_host = 5;

    // Port number of triton inference server that serves the Wav2VecTriton model
    int64 triton_server_port = 6;
}

// <p>PtFiles contains information about PT files.</p>
message PtFiles {

    // Path to the PT files.
    string path = 1;

    // Step for the PT files.
    string step = 2;

}

// <p>CkptFile contains information about checkpoint files.</p>
message CkptFile {

    // Path to the checkpoint file.
    string path = 1;

}


// <p>LanguageModels contains information about language models.</p>
message LanguageModels {

    // Path to the directory of language models.
    string path = 1;

    // Beam size for the search algorithm.
    int64 beam_size = 2;

    // Default language model to be selected if none is given.
    string default_lm = 3;

    // Weight for the language model scorer (alpha).
    float beam_search_scorer_alpha = 4;

    // Weight for the word insertion penalty (beta).
    float beam_search_scorer_beta = 5;

}

// <p>StreamingServer contains information about the streaming server.</p>
message StreamingServer {

    // Hostname of the streaming server.
    string host = 1;

    // Port number of the streaming server.
    int64 port = 2;

    // Output style for the streaming server.
    string output_style = 3;

    // Configuration for streaming speech recognition.
    StreamingSpeechRecognition streaming_speech_recognition = 4;

}

// <p>StreamingSpeechRecognition contains information about streaming speech recognition settings.</p>
message StreamingSpeechRecognition {

    // Indicates whether to transcribe non-final results.
    bool transcribe_not_final = 1;

    // Decoding method for speech recognition.
    string decoding_method = 2;

    // Sampling rate for audio input.
    int64 sampling_rate = 3;

    // Minimum audio chunk size for processing.
    int64 min_audio_chunk_size = 4;

    // Timeout between audio chunks; if exceeded, the stream will be stopped.
    float next_chunk_timeout = 5;

    // Configuration of the options to turn-detection in utterances
    TurnDetectionOptions turn_detection = 6;

}


// <p>Configuration of the options to turn-detection in utterances</p>
message TurnDetectionOptions {

    // Optional. Indicates if the turn-detection feature is active.
    optional bool active = 1;

    // Optional. Whether to transcribe the whole utterance when turn moment is detected. It is helpful to increase  
    // accuracy of transcriptions in cost of drop in speed. If deactivated, it just transcribe from last short silence 
    // period and concatenates the transcriptions of small audio chunks between tiny silences.
    optional bool full_utterance_deployment = 2;

    // Optional. Host name or IP address of the server that serves the LLM for turn-detection purpose.
    optional string llm_host = 3;

    // Optional. Port number of the server that serves the LLM for turn-detection purpose.
    optional int32 llm_port = 4;

    // Optional. Duration of request timeout in seconds to get result of request to LLM for turn-detection purpose. 
    // If the timeout occurs, result of turn-detection considered as False.
    optional float llm_request_timeout = 5;

}

// <p>VoiceActivityDetection contains information about voice activity detection settings.</p>
message VoiceActivityDetection {

    // Indicates if voice activity detection is active.
    string active = 1;

    // Sampling rate for voice activity detection.
    int64 sampling_rate = 2;

    // Configuration for the Pyannote model.
    Pyannote pyannote = 3;

}

// <p>Pyannote contains configuration for the Pyannote voice activity detection model.</p>
// <p>Library: <a href="https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb">pyannote-audio</a></p>
message Pyannote {

    // Full name of the Pyannote model.
    string model_name = 1;

    // Minimum audio size for processing.
    int64 min_audio_size = 2;

    // Fill inactive regions shorter than that many seconds.
    // Example <a href="https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb">notebook</a>
    float min_duration_off = 3;

    // Remove active regions shorter than that many seconds.
    // Example <a href="https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb">notebook</a>
    float min_duration_on = 4;

    // Host name of triton inference server that serves the Pyannote model
    string triton_server_host = 5;

    // Port number of triton inference server that serves the Pyannote model
    int64 triton_server_port = 6;

}

// <p>PostProcessing contains the configuration for post-processing.</p>
message PostProcessing {

    // List of names of active post-processors.
    repeated string pipeline = 1;

    // Post-processor configurations.
    PostProcessors post_processors = 2;

}

// <p>PostProcessors contains configurations for post-processors.</p>
message PostProcessors {

    // Configuration of the SymSpell spelling correction.
    SymSpell sym_spell = 1;

    // Configuration of the normalization object.
    S2tNormalization normalization = 2;

    // Configuration of the LLM post-processing.
    S2tLlmPostProcessing llm_post_processing = 3;

}

// <p>SymSpell contains configuration for the SymSpell spelling correction.</p>
message SymSpell {

    // Path to the dictionary used by symspell
    string dict_path = 1;

    // The maximal edit-distance to consider for spelling correction (affects performance - bigger number
    // takes longer to process!)
    int64 max_dictionary_edit_distance = 2;

    // The length (number of characters) of the prefix to consider for filtering
    int64 prefix_length = 3;

}

// <p>S2tNormalization contains configuration for the speech-to-text normalization.</p>
message S2tNormalization {
    
    // Language for normalization of transcriptions.
    string language = 1;
    
    // List of names of active normalizations.
    repeated string pipeline = 2;
    
}

// <p>S2tLlmPostProcessing contains configuration for the speech-to-text postprocessing with LLM.</p>
message S2tLlmPostProcessing {

    // Optional. Host name or IP address of the server that serves the LLM for post-processing purpose.
    optional string llm_host = 1;

    // Optional. Port number of the server that serves the LLM for post-processing purpose.
    optional int32 llm_port = 2;

    // Optional. Duration of request timeout in seconds to get result of request to LLM for post-processing purpose. 
    // If the timeout occurs, result of post-processing returns the input text with no change.
    optional float llm_request_timeout = 3;

    // Optional. Configuration of the options to casing task in LLM post-processing.
    optional S2tLlmPostProcessingCasingOptions s2t_llm_post_processing_casing_options = 4;

    // Optional. Configuration of the options to punctuation task in LLM post-processing.
    optional S2tLlmPostProcessingPunctuationOptions s2t_llm_post_processing_punctuation_options = 5;

    // Optional. Configuration of the options to spelling-correction task in LLM post-processing.
    optional S2tLlmPostProcessingSpellCorrectionOptions s2t_llm_post_processing_spelling_correction_options = 6;

    // Optional. Configuration of the options to semantic-correction task in LLM post-processing.
    optional S2tLlmPostProcessingSemanticCorrectionOptions s2t_llm_post_processing_semantic_correction_options = 7;

    // Optional. Configuration of the options to translation task in LLM post-processing.
    optional S2tLlmPostProcessingTranslationOptions s2t_llm_post_processing_translation_options = 8;

    // Optional. Configuration of the options to inverse-normalization task in LLM post-processing.
    optional S2tLlmPostProcessingInverseNormalizationOptions s2t_llm_post_processing_inverse_normalization_options = 9;

    // Optional. Configuration of the options to normalization task in LLM post-processing.
    optional S2tLlmPostProcessingNormalizationOptions s2t_llm_post_processing_normalization_options = 10;

    // Optional. Configuration of the options to summarization task in LLM post-processing.
    optional S2tLlmPostProcessingSummarizationOptions s2t_llm_post_processing_summarization_options = 11;

    // Optional. Configuration of the options to user-prompt task in LLM post-processing.
    optional S2tLlmPostProcessingUserPromptOptions s2t_llm_post_processing_user_prompt_options = 12;

}

// <p>Configuration of the options to casing task in LLM post-processing.</p>
message S2tLlmPostProcessingCasingOptions {

    // Optional. Indicates if the casing task of LLM post-processing is active.
    optional bool active = 1;

}

// <p>Configuration of the options to punctuation task in LLM post-processing.</p>
message S2tLlmPostProcessingPunctuationOptions {

    // Optional. Indicates if the punctuation task of LLM post-processing is active.
    optional bool active = 1;

}

// <p>Configuration of the options to spelling-correction task in LLM post-processing.</p>
message S2tLlmPostProcessingSpellCorrectionOptions {

    // Optional. Indicates if the spelling-correction task of LLM post-processing is active.
    optional bool active = 1;

}

// <p>Configuration of the options to semantic-correction task in LLM post-processing.</p>
message S2tLlmPostProcessingSemanticCorrectionOptions {

    // Optional. Indicates if the semantic-correction task of LLM post-processing is active.
    optional bool active = 1;

}

// <p>Configuration of the options to translation task in LLM post-processing.</p>
message S2tLlmPostProcessingTranslationOptions {

    // Optional. Indicates if the translation task of LLM post-processing is active.
    optional bool active = 1;

    // Optional. Target language of the translation task of LLM post-processing.
    optional string language = 2;

}

// <p>Configuration of the options to inverse-normalization task in LLM post-processing.</p>
message S2tLlmPostProcessingInverseNormalizationOptions {

    // Optional. Indicates if the inverse-normalization task of LLM post-processing is active.
    optional bool active = 1;

    // Optional. Indicates if inverse-normalization of email address sub-task of LLM post-processing is active.
    optional bool email = 2;

    // Optional. Indicates if inverse-normalization of phone number sub-task of LLM post-processing is active.
    optional bool phone_number = 3;

    // Optional. Indicates if inverse-normalization of date and time sub-task of LLM post-processing is active.
    optional bool date_and_time = 4;

    // Optional. Indicates if inverse-normalization of credit card number sub-task of LLM post-processing is active.
    optional bool credit_card_number = 5;

    // Optional. Indicates if inverse-normalization of social security number sub-task of LLM post-processing is active
    optional bool social_security_number = 6;

    // Optional. Indicates if inverse-normalization of time zone sub-task of LLM post-processing is active.
    optional bool time_zone = 7;

}

// <p>Configuration of the options to normalization task in LLM post-processing.</p>
message S2tLlmPostProcessingNormalizationOptions {

    // Optional. Indicates if the normalization task of LLM post-processing is active.
    optional bool active = 1;

    // Optional. Indicates if normalization of email address sub-task of LLM post-processing is active.
    optional bool email = 2;
    
    // Optional. Indicates if normalization of phone number sub-task of LLM post-processing is active.
    optional bool phone_number = 3;
    
    // Optional. Indicates if normalization of date and time sub-task of LLM post-processing is active.
    optional bool date_and_time = 4;

    // Optional. Indicates if normalization of credit card number sub-task of LLM post-processing is active.
    optional bool credit_card_number = 5;

    // Optional. Indicates if normalization of social security number sub-task of LLM post-processing is active
    optional bool social_security_number = 6;
    
    // Optional. Indicates if normalization of time zone sub-task of LLM post-processing is active.
    optional bool time_zone = 7;
    
}

// <p>Configuration of the options to summarization task in LLM post-processing.</p>
message S2tLlmPostProcessingSummarizationOptions {

    // Optional. Indicates if the summarization task of LLM post-processing is active.
    optional bool active = 1;

    // Optional. Minimum number of characters of the summary generated in summarization task of LLM post-processing.
    optional int32 min_chars = 2;

    // Optional. Maximum number of characters of the summary generated in summarization task of LLM post-processing.
    optional int32 max_chars = 3;

}

// <p>Configuration of the options to user-prompt task in LLM post-processing.</p>
message S2tLlmPostProcessingUserPromptOptions {

    // Optional. Indicates if the user-prompt task of LLM post-processing is active. This task overwrites
    optional bool active = 1;

    // Optional. The prompt to give LLM directly for post-processing purpose.
    optional string prompt = 2;

}

// <p>Logging contains configuration for logging.</p>
message Logging {

    // Type of logging.
    string type = 1;

    // Path for logging.
    string path = 2;

}

///////////////////////////////////////////
// GET LIST OF AVAILABLE LANGUAGE MODELS //
///////////////////////////////////////////

// <p>ListS2tLanguageModelsRequest is used to request a list of available language models for specified pipelines.</p>
message ListS2tLanguageModelsRequest {

    // List of pipeline IDs to retrieve their available language models.
    // Example: [&quot;pipeline_1&quot;, &quot;pipeline_2&quot;]
    repeated string ids = 1;

}

// <p>LanguageModelPipelineId contains information about a pipeline and its available language models.</p>
message LanguageModelPipelineId {

    // A pipeline ID. Example: &quot;pipeline_1&quot;
    string pipeline_id = 1;

    // A list of all available language models for the corresponding pipeline ID. Example: [&quot;model_1&quot;, &quot;model_2&quot;]
    repeated string model_names = 2;

}

// <p>ListS2tLanguageModelsResponse is used to return the available language models for specified pipelines.</p>
message ListS2tLanguageModelsResponse {

    // Response is a list of <code>LanguageModelPipelineId</code>, where each element contains a pipeline ID and its associated
    // language models.
    // Example: [{pipeline_id: &quot;pipeline_1&quot;, model_names: [&quot;model_1&quot;, &quot;model_2&quot;]}, {pipeline_id: &quot;pipeline_2&quot;,
    // model_names: [&quot;model_3&quot;]}]
    repeated LanguageModelPipelineId lm_pipeline_ids = 1;

}

/////////////////////////////////////
// CUSTOM LANGUAGE MODEL FEATURES  //
/////////////////////////////////////

// <p>CreateUserLanguageModelRequest is used to request the creation of a new user-specific language model.</p>
message CreateUserLanguageModelRequest {

    // Name of the language model to create. Example: &quot;user_lm_1&quot;
    string language_model_name = 1;

}

// <p>DeleteUserLanguageModelRequest is used to request the deletion of a user-specific language model.</p>
message DeleteUserLanguageModelRequest {

    // Name of the language model to delete. Example: &quot;user_lm_1&quot;
    string language_model_name = 1;

}

// <p>AddDataToUserLanguageModelRequest is used to request the addition of data to a user-specific language model.</p>
message AddDataToUserLanguageModelRequest {

    // Name of the language model to which to add data. Example: &quot;user_lm_1&quot;
    string language_model_name = 1;

    // Zip file containing data in the form of text files.
    // Example: A zip file with text files containing sentences or phrases in the target language.
    bytes zipped_data = 2;

}

// <p>TrainUserLanguageModelRequest is used to request the training of a user-specific language model.</p>
message TrainUserLanguageModelRequest {

    // Name of the language model to train. Example: &quot;user_lm_1&quot;
    string language_model_name = 1;

    // Order n of the ngram. Example: 3 (for trigram model)
    int64 order = 2;

}

////////////////////////
// LIST OF S2T NORMALIZATION PIPELINES //
////////////////////////

// <p>The request message for <code>ListS2tNormalizationPipelines</code>.</p>
// <p>Filter pipelines by attributes in request.</p>
message ListS2tNormalizationPipelinesRequest {

    // Optional. Define the language.
    string language = 1;

}

// <p>The response message for <code>ListS2tNormalizationPipelines</code>.</p>
message ListS2tNormalizationPipelinesResponse {

    // Required. Representation of a list of normalization pipelines configurations.
    // Retrieved by <code>ListS2tNormalizationPipelines</code>, containing the configurations of
    // normalization pipelines with the specifications received in the <code>ListS2tNormalizationPipelinesRequest</code>.
    repeated string s2t_normalization_pipelines = 1;

}